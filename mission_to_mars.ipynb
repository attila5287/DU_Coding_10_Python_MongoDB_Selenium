{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Scraping\n",
    "* Complete your initial scraping using Jupyter Notebook, BeautifulSoup, Pandas, and Requests/Splinter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print multiple output in jpynb\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'splinter' from 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\splinter\\\\__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'selenium' from 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\selenium\\\\__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<function splinter.browser.Browser(driver_name='firefox', *args, **kwargs)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<module 'selenium.webdriver' from 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\selenium\\\\webdriver\\\\__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Dependencies\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import splinter\n",
    "from splinter import Browser\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "import time\n",
    "BeautifulSoup\n",
    "splinter\n",
    "selenium\n",
    "Browser\n",
    "webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Scrape the NASA Mars News Site and collect the latest News Title and Paragraph Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find('div', class_='features')\n",
    "#  - - \n",
    "# for result in results:\n",
    "#     print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Opportunity Hunkers Down During Dust Storm'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"It's the beginning of the end for the planet-encircling dust storm on Mars. But it could still be weeks, or even months, before skies are clear enough for NASA's Opportunity rover to recharge its batteries and phone home.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_title_raw = results.find('div', class_='content_title').text\n",
    "news_paragraph_raw = results.find('div', class_='rollover_description').text\n",
    "news_title = news_title_raw.strip()\n",
    "news_paragraph = news_paragraph_raw.strip()\n",
    "news_title\n",
    "news_paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test chromedriver and selenium-webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Expected Driver Test Performance:<br>[ ] shows google  <br>[ ] sending keys to search box <br>[ ] shows Denver weather today search results<br>[ ] shows github repo for the code<br>[ ] quits application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriverexe_path = 'C:/Users/DENVER/Documents/GitHub/DU_BigData_10_MongoDB_Flask_API/chromedriver.exe'\n",
    "driver = webdriver.Chrome(chromedriverexe_path)\n",
    "driver.get('http://www.google.com/xhtml');\n",
    "time.sleep(1) \n",
    "search_box = driver.find_element_by_name('q')\n",
    "search_box.send_keys('Denver Weather Today:')\n",
    "search_box.submit()\n",
    "time.sleep(1) \n",
    "driver.get('http://github.com/attila5287/DU_Coding_10_Python_MongoDB_Selenium')\n",
    "time.sleep(3) \n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Chromedriver setup checklist if code fails test: <br> [ ] add as PATH variable<br> [ ] leading slash in path string<br> [ ] upgrade latest Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test splinter Browser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [ ] shows authors github profile and repo for code <br>\n",
    "[ ] types splinter-python acceptance testing for web applications on searchbox<br>\n",
    "[ ] quits browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'C:/Users/DENVER/Documents/GitHub/DU_BigData_10_MongoDB_Flask_API/chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "browser.visit('http://www.github.com/attila5287')\n",
    "time.sleep(1)\n",
    "browser.visit('http://github.com/attila5287/DU_Coding_10_Python_MongoDB_Selenium')\n",
    "time.sleep(3)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visit the url for JPL Featured Space Image [here](https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nasa_url = 'http://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Selenium module to reach website via chrome browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriverexe_path = 'C:/Users/DENVER/Documents/GitHub/DU_BigData_10_MongoDB_Flask_API/chromedriver.exe'\n",
    "driver = webdriver.Chrome(chromedriverexe_path) \n",
    "executable_path = {'executable_path': 'C:/Users/DENVER/Documents/GitHub/DU_BigData_10_MongoDB_Flask_API/chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "browser.visit(nasa_url) \n",
    "nasa_html = browser.html\n",
    "browser.quit()\n",
    "# print(nasa_html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Splinter module to reach website via chrome browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an alternative way might be necessary should splinter give error like above: \n",
    "# Selenium(.page_source) method\n",
    "chromedriverexe_path = 'C:/Users/DENVER/Documents/GitHub/DU_BigData_10_MongoDB_Flask_API/chromedriver.exe'\n",
    "driver = webdriver.Chrome(chromedriverexe_path) \n",
    "driver.get(nasa_url) \n",
    "nasa_html_alt = driver.page_source\n",
    "nasa_soup = BeautifulSoup(nasa_html_alt, \"lxml\")\n",
    "# nasa_html_alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called `featured_image_url`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spaceimages/images/mediumsize/PIA17896_ip.jpg'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect page >elements through google chrome UI to find accurate parameters\n",
    "featured = nasa_soup.find('div', class_='default floating_text_area ms-layer')\n",
    "featured_image = featured.find('footer')\n",
    "# featured\n",
    "# featured_image\n",
    "# featured_image.find('a')\n",
    "# featured_image.find('a')[class_ = 'data-fancybox-href']\n",
    "featured_image.find('a')['data-fancybox-href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spaceimages/images/mediumsize/PIA17896_ip.jpg'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "append_url = featured_image.find('a')['data-fancybox-href']\n",
    "append_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make sure to find the image url to the full size `.jpg` image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://www.jpl.nasa.gov'\n",
    "base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Make sure to save a complete url string for this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov/spaceimages/images/mediumsize/PIA17896_ip.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featured_image_url = base_url + append_url\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visits via browser from splinter <br>\n",
    "> Saves a local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executable_path = {'executable_path': 'C:/Users/DENVER/Documents/GitHub/DU_BigData_10_MongoDB_Flask_API/chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "# via splinter browser method\n",
    "browser.visit(featured_image_url)\n",
    "browser.driver.save_screenshot('Figure1_FeaturedImage.png')\n",
    "time.sleep(3)\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mars Weather @twitter\n",
    "* Visit the Mars Weather twitter account [here](https://twitter.com/marswxreport?lang=en) and scrape the latest Mars weather tweet from the page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executable_path = {'executable_path': 'C:/Users/DENVER/Documents/GitHub/DU_BigData_10_MongoDB_Flask_API/chromedriver.exe'}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n",
    "# via splinter browser method\n",
    "browser.visit('https://twitter.com/marswxreport?lang=en')\n",
    "marsweather_html = browser.html\n",
    "# browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_soup = BeautifulSoup(marsweather_html, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save the tweet text for the weather report as a variable called `mars_weather`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .find will bring the top element on the stack\n",
    "mars_weather=twitter_soup.find('p', 'tweet-text').text \n",
    "mars_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mars Facts\n",
    "  * Visit the Mars Facts webpage [here](http://space-facts.com/mars/) and use <b>Pandas</b> to scrape the table containing facts about the planet including Diameter, Mass, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape space-facts.com for mars fact using Pandas read_html function\n",
    "mars_facts_url = 'https://space-facts.com/mars/'\n",
    "tables = pd.read_html(mars_facts_url)\n",
    "tables #prints as list\n",
    "type(tables) #list\n",
    "len(tables) #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  still no dataframe: Pandas returned an iterable list\n",
    "# Pandas returned a dataframe (list of dictionaries)\n",
    "print('- - - - tables[0]- - - - ')\n",
    "tables[0] #prints as dataframe\n",
    "type(tables[0]) # pandas.core.frame.DataFrame\n",
    "# it is a dataframe!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# however using pd.DataFrame will be safe\n",
    "marsfacts_df = pd.DataFrame(tables[0])\n",
    "marsfacts_df = marsfacts_df.rename(columns = {0:'Description', 1:'Value'})\n",
    "marsfacts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marsfacts_df = marsfacts_df.set_index('Description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm Desc dropped to index\n",
    "marsfacts_df.columns\n",
    "# returns Value only, confirmed! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Pandas to convert the data to a HTML table string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marsfacts_df2html = marsfacts_df.to_html().replace('\\n','')\n",
    "# type(marsfacts_df2html) # returns string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Mars Hemispheres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Visit the USGS Astrogeology site [here](https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars) to obtain high resolution images for each of Mar's hemispheres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape astrogeology.usgs.gov for hemisphere image urls and titles\n",
    "hemisphere_url = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(hemisphere_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_html = browser.html\n",
    "hemisphere_soup = bs(hemisphere_html, 'lxml')\n",
    "base_url =\"https://astrogeology.usgs.gov\"\n",
    "\n",
    "image_list = hemisphere_soup.find_all('div', class_='item')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save both the image url string for:\n",
    " * Full resolution hemisphere image\n",
    " * Hemisphere title containing the hemisphere name\n",
    "   * Use a Python dictionary to store the data using the keys `img_url` and `title`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You will need to click each of the links to the hemispheres in order to find the image url to the full resolution image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each hemisphere and click on link to find large resolution image url\n",
    "for image in image_list:\n",
    "    hemisphere_dict = {}\n",
    "    \n",
    "    href = image.find('a', class_='itemLink product-item')\n",
    "    link = base_url + href['href']\n",
    "    browser.visit(link)\n",
    "    \n",
    "    time.sleep(1)\n",
    "    \n",
    "    hemisphere_html2 = browser.html\n",
    "    hemisphere_soup2 = bs(hemisphere_html2, 'lxml')\n",
    "    \n",
    "    img_title = hemisphere_soup2.find('div', class_='content').find('h2', class_='title').text\n",
    "    hemisphere_dict['title'] = img_title\n",
    "    \n",
    "    img_url = hemisphere_soup2.find('div', class_='downloads').find('a')['href']\n",
    "    hemisphere_dict['url_img'] = img_url\n",
    "    \n",
    "# Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "\n",
    "    hemisphere_image_urls.append(hemisphere_dict)\n",
    "      \n",
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    " ---the end--- <br> \n",
    " ### Step 1:Web Scraping completed.<br> \n",
    " Next... \n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Step_2: <br>\n",
    ">  Data Storage with MongoDB and Web Framework with Flask<br>\n",
    "<br>\n",
    ">  Description detailed below; code in repo as below...\n",
    "* please click link below and follow app.py for Step 2\n",
    ">    [link_to_repo](https://github.com/attila5287/DU_Coding_10_Python_MongoDB_Selenium) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: MongoDB and Flask App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use MongoDB with Flask templating to create a new HTML page that displays all of the information that was scraped from the URLs above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Start by converting your Jupyter notebook into a Python script called `scrape_mars.py` with a function called `scrape` that will execute all of your scraping code from above and return one Python dictionary containing all of the scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, create a route called `/scrape` that will import your `scrape_mars.py` script and call your `scrape` function.\n",
    "\n",
    "     * Store the return value in Mongo as a Python dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a root route `/` that will query your Mongo database and pass the mars data into an HTML template to display the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a template HTML file called `index.html` that will take the mars data dictionary and display all of the data in the appropriate HTML elements. Use the following as a guide for what the final product should look like, but feel free to create your own design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Pymongo for CRUD applications for your database. For this homework, you can simply overwrite the existing document each time the `/scrape` url is visited and new data is obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use Bootstrap to structure your HTML template."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
